{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a396eb4",
   "metadata": {},
   "source": [
    "Generatively Pretrained Transformer (GPT)\n",
    "https://arxiv.org/pdf/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9a853",
   "metadata": {},
   "source": [
    "<h3> 1. Import Libraries </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd1f6530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.16\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4931eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# list of modules to install and their corresponding names\n",
    "modules = {'tensorflow': 'tf', 'numpy': 'np', 'pandas': 'pd', 'requests': 'requests',\n",
    "           'tiktoken': 'tiktoken', 'openai': 'openai', 'torch': 'torch', 'torch.nn': 'nn', 'matplotlib.pyplot': 'plt', 'torch.nn.functional': 'F'}\n",
    "\n",
    "# iterate over the modules and install them\n",
    "for module, name in modules.items():\n",
    "    try:\n",
    "        exec(f\"import {module} as {name}\")\n",
    "    except ModuleNotFoundError as error:\n",
    "        print(f\"{error.name} module not found. Installing {error.name}...\")\n",
    "        subprocess.check_call([\"pip\", \"install\", error.name])\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be65d0",
   "metadata": {},
   "source": [
    "<b> Check for available GPUs for Training </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e07794d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for GPUs\n",
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a9b97",
   "metadata": {},
   "source": [
    "<h3> 2. Import Data </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4d5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def load_text_from_url(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    text = response.read().decode('utf-8')\n",
    "    return text\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "text = load_text_from_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1510c",
   "metadata": {},
   "source": [
    "<h3> 3. Basic Data Exploration </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461987b9",
   "metadata": {},
   "source": [
    "<h3> 4. Create Basic Tokenization Functions and Encode Data </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee01a9c",
   "metadata": {},
   "source": [
    "<b> Encoding 1: Character Level </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c487e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from characters to integers\n",
    "#stoi = { ch:i for i,ch in enumerate(chars) } # This creates a lookup table for converting a character to an int\n",
    "#itos = { i:ch for i,ch in enumerate(chars) } # This creates a lookup table for converting an int to a char\n",
    "#encode = lambda s: [stoi[c] for c in s]\n",
    "#decode = lambda l: ''.join(itos[i] for i in l)\n",
    "#print(encode(\"Hello World\"))\n",
    "#print(decode(encode(\"Hello World\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf04dc",
   "metadata": {},
   "source": [
    "<b> Encoding 2: Subword OpenAI GPT4 Dictionary 'cl100k_base'</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf6cf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the gpt2 encoding dictionary is 50257\n",
      "[15496, 2159]\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "# open ai tokenizer function goes here (tiktoken)\n",
    "gpt2enc = tiktoken.get_encoding('gpt2')\n",
    "# gpt4enc = tiktoken.get_encoding('cl100k_base')\n",
    "vocab_size=gpt2enc.n_vocab\n",
    "print(f\"The length of the gpt2 encoding dictionary is {vocab_size}\")\n",
    "print(gpt2enc.encode(\"Hello World\"))\n",
    "print(gpt2enc.decode(gpt2enc.encode(\"Hello World\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6deb4486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence piece from google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0683d0",
   "metadata": {},
   "source": [
    "<h3> 5. Encode Dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba521886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([338025]) torch.int64\n",
      "tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
      "         3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
      "          461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
      "         1639,   389,   477, 12939,  2138,   284,  4656,   621,   284,  1145,\n",
      "          680,    30,   198,   198,  3237,    25,   198,  4965,  5634,    13,\n",
      "        12939,    13,   198,   198,  5962, 22307,    25,   198,  5962,    11,\n",
      "          345,   760,   327,  1872,   385,  1526, 28599,   318,  4039,  4472,\n",
      "          284,   262,   661,    13,   198,   198,  3237,    25,   198,  1135,\n",
      "          760,   470,    11,   356,   760,   470,    13,   198,   198,  5962,\n",
      "        22307,    25,   198,  5756,   514,  1494,   683,    11,   290,   356])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(gpt2enc.encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1c0d5",
   "metadata": {},
   "source": [
    "<h3> 6. Create Training and Validation Datasets </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f57e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_split(dataset,cutoff):\n",
    "    n = int(cutoff*len(dataset))\n",
    "    train_data = dataset[:n]\n",
    "    val_data = dataset[n:]\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ad22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.90\n",
    "train_data, val_data = train_validate_split(data,cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9d40c",
   "metadata": {},
   "source": [
    "<h3> 7. Define Basic Bigram Transformer Model </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31986745-d699-4774-8bc4-3e616a1cde37",
   "metadata": {},
   "source": [
    "A Bigram transformer is a type of language model that uses the concept of bigrams to predict the next word in a sequence of words. Bigrams are pairs of adjacent words in a sentence or text. For example, in the sentence \"The quick brown fox jumps over the lazy dog,\" the bigrams are \"The quick\", \"quick brown\", \"brown fox\", \"fox jumps\", \"jumps over\", \"over the\", and \"the lazy\".\n",
    "\n",
    "A Bigram transformer learns the probability distribution of bigrams from a given text corpus, and uses this knowledge to predict the most likely next word in a sentence based on the previous word. It is a simple and effective approach for language modeling that can be used for a variety of natural language processing tasks, such as machine translation, speech recognition, and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8eba86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f12b4faf90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 #256 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 50\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "patience = 3\n",
    "plateau = 0.005\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75e994",
   "metadata": {},
   "source": [
    "<b> Data Batching Function </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf91db",
   "metadata": {},
   "source": [
    "This function is used for generating a mini-batch of data from the specified dataset (training or validation) to be used during the training or evaluation process of a machine learning model. It randomly selects batch_size starting indices, then creates input sequences (x) and their corresponding target sequences (y) of length block_size. The input and target sequences are shifted by one position, making the model predict the next element in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daa16676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    This function generates a small batch of data consisting of inputs (x) and targets (y) from the training or validation dataset.\n",
    "    \n",
    "    Args:\n",
    "    split (str): A string that indicates whether to use the training dataset or the validation dataset. It accepts two values, 'train' or 'val'.\n",
    "    \n",
    "    Returns:\n",
    "    x (torch.Tensor): A tensor containing the input sequences for the mini-batch. The shape of the tensor is (batch_size, block_size).\n",
    "    y (torch.Tensor): A tensor containing the target sequences for the mini-batch. The shape of the tensor is (batch_size, block_size).\n",
    "    \n",
    "    Global Variables:\n",
    "    train_data (torch.Tensor): A tensor containing the training dataset.\n",
    "    val_data (torch.Tensor): A tensor containing the validation dataset.\n",
    "    batch_size (int): The number of samples in a single batch.\n",
    "    block_size (int): The length of each input and target sequence.\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252dca8a",
   "metadata": {},
   "source": [
    "<b> Loss Estimation Function </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6704c08-a280-46b9-816e-0e410beba0fc",
   "metadata": {},
   "source": [
    "<h4>Function Name:</h4> \n",
    "<p><code>estimate_loss()</code></p>\n",
    "<h4>Input:</h4> \n",
    "<p>None</p>\n",
    "<h4>Output:</h4> \n",
    "<p>A Python dictionary containing the average loss over the training and validation datasets.</p>\n",
    "<h4>Functionality:</h4> \n",
    "<p>This function estimates the average loss over the training and validation datasets for a PyTorch model.</p>\n",
    "<h4>Steps:</h4> \n",
    "<ol>\n",
    "    <li>Initializes an empty Python dictionary <code>out</code> to store the average loss for each dataset.</li>\n",
    "    <li>Sets the model to evaluation mode using <code>model.eval()</code>.</li>\n",
    "    <li>Loops over the two splits of the dataset: training and validation.</li>\n",
    "    <li>Initializes a PyTorch tensor <code>losses</code> with shape <code>(eval_iters,)</code> to store the loss for each evaluation iteration.</li>\n",
    "    <li>Loops over <code>eval_iters</code> number of iterations and retrieves a batch of data for the given split using <code>get_batch(split)</code>.</li>\n",
    "    <li>Passes the input data <code>X</code> and target <code>Y</code> through the PyTorch model <code>model</code> to get the logits and the loss for the batch.</li>\n",
    "    <li>Stores the loss value as a scalar in the <code>losses</code> tensor at the index <code>k</code>.</li>\n",
    "    <li>Computes the mean of the <code>losses</code> tensor and stores the result in the <code>out</code> dictionary with the key being the split.</li>\n",
    "    <li>Sets the model back to training mode using <code>model.train()</code>.</li>\n",
    "    <li>Returns the <code>out</code> dictionary containing the average loss for each dataset.</li>\n",
    "</ol>\n",
    "<p><em>Note: It is assumed that the variables <code>model</code>, <code>eval_iters</code>, and <code>get_batch()</code> are defined and accessible within the scope of this function.</em></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47206960",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # tells pytorch we will not call \"backward\" on the function (back propagation)\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1203cf-7cd3-4a15-8680-89d488c832ce",
   "metadata": {},
   "source": [
    "\n",
    "<h4><code>class Head(nn.Module):</code></h4>\n",
    "<p>\n",
    "In natural language processing, self-attention is a technique used in deep learning models to help the model understand the context of a sentence or a document. It works by assigning weights to each word in the input text, based on the relevance of that word to the other words in the text. These weights help the model to focus on the most important words and ignore the less important ones.\n",
    "\n",
    "A head of self-attention is a single part of the self-attention mechanism. Think of it like a small flashlight that helps the model to focus on a specific part of the input text. Each head of self-attention works independently to assign weights to the words in the input text, and then combines the weighted representation of the input text to create a more accurate understanding of the context.\n",
    "\n",
    "Just like how you might use multiple flashlights to illuminate different parts of a room, deep learning models can use multiple heads of self-attention to better understand different aspects of the input text. By combining the information from multiple heads of self-attention, the model can create a more complete and accurate representation of the input text, which helps it perform better on natural language processing tasks like language translation and text classification.</p>\n",
    "<h4><code>Parameters:</code></h4>\n",
    "<ul>\n",
    "<li><code>head_size (int)</code>: The size of the subspace for key, query, and value tensors.</li>\n",
    "</ul>\n",
    "<h4><code>Attributes:</code></h4>\n",
    "<ul>\n",
    "<li><code>key (nn.Linear)</code>: Linear layer for projecting the input tensor into the key subspace.</li>\n",
    "<li><code>query (nn.Linear)</code>: Linear layer for projecting the input tensor into the query subspace.</li>\n",
    "<li><code>value (nn.Linear)</code>: Linear layer for projecting the input tensor into the value subspace.</li>\n",
    "<li><code>tril (torch.Tensor)</code>: Lower-triangular matrix of ones, with the same size as the sequence length, used to mask out attention scores corresponding to positions that have not been seen yet during training.</li>\n",
    "<li><code>dropout (nn.Dropout)</code>: Dropout layer to prevent overfitting.</li>\n",
    "</ul>\n",
    "<h4><code>Inputs:</code></h4>\n",
    "<ul>\n",
    "<li><code>x (torch.Tensor)</code>: Input tensor of shape `(B, T, C)`, where `B` is the batch size, `T` is the sequence length, and `C` is the hidden size of the input tensor.</li>\n",
    "</ul>\n",
    "<h4><code>Outputs:</code></h4>\n",
    "<ul>\n",
    "<li><code>out (torch.Tensor)</code>: Output tensor of the same shape as the input tensor.</li>\n",
    "</ul>\n",
    "<h4><code>Methods:</code></h4>\n",
    "<ul>\n",
    "<li><code>forward(x: torch.Tensor) -&gt; torch.Tensor:</code> Computes the self-attention of the input tensor `x`, and returns the output tensor.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5478053-ec37-4c89-a8db-19b24053478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model copied from https://github.com/karpathy/nanoGPT\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d9175-1374-4f3b-9952-9d3b72180e92",
   "metadata": {},
   "source": [
    "<div>\n",
    "<h3><code>class MultiHeadAttention(nn.Module):</code></h3>\n",
    "<p>The MultiHeadAttention module is an implementation of the multi-head attention mechanism used in transformer models. The idea behind multi-head attention is to split the input tensor into multiple \"heads\", and apply the self-attention mechanism to each head independently. This allows the model to attend to different parts of the input tensor with different sets of weights, which can help the model learn more diverse and meaningful representations.\n",
    "\n",
    "The MultiHeadAttention module takes an input tensor x of shape (B, T, C), where B is the batch size, T is the sequence length, and C is the hidden size of the input tensor. The num_heads parameter specifies the number of attention heads, and the head_size parameter specifies the size of each attention head.\n",
    "\n",
    "In the __init__ method, the MultiHeadAttention module creates a list of num_heads Head modules, each with a size of head_size. Each Head module computes the attention scores and corresponding weighted values for a single attention head. The MultiHeadAttention module then applies a linear projection to the concatenated output of the attention heads to transform it back into the original hidden size C. Finally, a dropout layer is applied to prevent overfitting.\n",
    "\n",
    "In the forward method, the MultiHeadAttention module passes the input tensor x through each Head module, concatenates the outputs along the last dimension, and applies the linear projection and dropout layers to the concatenated output. The resulting tensor has the same shape as the input tensor.\n",
    "\n",
    "Overall, the MultiHeadAttention module allows the transformer model to attend to multiple parts of the input tensor at once, and learn more complex relationships between different parts of the input. This can lead to better performance on tasks that require modeling complex relationships between different parts of the input, such as natural language understanding or image processing.</p>\n",
    "<h4><code>Parameters:</code></h4>\n",
    "<ul>\n",
    "<li><code>num_heads (int)</code>: The number of heads in the multi-head attention.</li>\n",
    "<li><code>head_size (int)</code>: The size of each head in the multi-head attention.</li>\n",
    "</ul>\n",
    "<h4><code>Attributes:</code></h4>\n",
    "<ul>\n",
    "<li><code>heads (nn.ModuleList)</code>: Module list containing the individual heads of self-attention.</li>\n",
    "<li><code>proj (nn.Linear)</code>: Linear layer for projecting the concatenated output of the individual attention heads into the original hidden size.</li>\n",
    "<li><code>dropout (nn.Dropout)</code>: Dropout layer to prevent overfitting.</li>\n",
    "</ul>\n",
    "<h4><code>Inputs:</code></h4>\n",
    "<ul>\n",
    "<li><code>x (torch.Tensor)</code>: Input tensor of shape `(B, T, C)`, where `B` is the batch size, `T` is the sequence length, and `C` is the hidden size of the input tensor.</li>\n",
    "</ul>\n",
    "<h4><code>Outputs:</code></h4>\n",
    "<ul>\n",
    "<li><code>out (torch.Tensor)</code>: Output tensor of the same shape as the input tensor.</li>\n",
    "</ul>\n",
    "<h4><code>Methods:</code></h4>\n",
    "<ul>\n",
    "<li><code>forward(x: torch.Tensor) -&gt; torch.Tensor:</code> Computes the multi-head self-attention of the input tensor `x`, and returns the output tensor.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd134baf-56b5-4649-837d-e784b4d813fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model copied from https://github.com/karpathy/nanoGPT\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e663690-c8cf-4d19-beba-e636d881f20c",
   "metadata": {},
   "source": [
    "<div>\n",
    "<h3><code>class FeedFoward(nn.Module):</code></h3>\n",
    "<p>The FeedFoward module is a two-layer feedforward neural network used in transformer models. It consists of a fully connected layer followed by a ReLU activation function, another fully connected layer, and a dropout layer. The module takes an input tensor x of shape (B, T, C), where B is the batch size, T is the sequence length, and C is the hidden size of the input tensor.\n",
    "\n",
    "In the __init__ method, the FeedFoward module creates a sequential neural network with two fully connected layers. The first layer has an output size of 4 * n_embd and uses the ReLU activation function. The second layer has an output size of n_embd. Finally, a dropout layer is applied to prevent overfitting.\n",
    "\n",
    "In the forward method, the FeedFoward module passes the input tensor x through the sequential neural network to compute the output tensor. The resulting tensor has the same shape as the input tensor.\n",
    "\n",
    "Overall, the FeedFoward module is used to add non-linearity and increase the expressiveness of the transformer model. The two-layer feedforward neural network can learn complex representations of the input tensor, which can be helpful for tasks that require modeling complex relationships between different parts of the input, such as natural language understanding or image processing.</p>\n",
    "<h4><code>Parameters:</code></h4>\n",
    "<ul>\n",
    "<li><code>n_embd (int)</code>: The size of the input and output tensors of the feedforward neural network.</li>\n",
    "</ul>\n",
    "<h4><code>Attributes:</code></h4>\n",
    "<ul>\n",
    "<li><code>net (nn.Sequential)</code>: Sequential neural network containing the fully connected layers and dropout layer of the feedforward neural network.</li>\n",
    "</ul>\n",
    "<h4><code>Inputs:</code></h4>\n",
    "<ul>\n",
    "<li><code>x (torch.Tensor)</code>: Input tensor of shape `(B, T, C)`, where `B` is the batch size, `T` is the sequence length, and `C` is the hidden size of the input tensor.</li>\n",
    "</ul>\n",
    "<h4><code>Outputs:</code></h4>\n",
    "<ul>\n",
    "<li><code>out (torch.Tensor)</code>: Output tensor of the same shape as the input tensor.</li>\n",
    "</ul>\n",
    "<h4><code>Methods:</code></h4>\n",
    "<ul>\n",
    "<li><code>forward(x: torch.Tensor) -&gt; torch.Tensor:</code> Computes the feedforward neural network of the input tensor `x`, and returns the output tensor.</li>\n",
    "</ul>\n",
    "<p>Overall, the `FeedFoward` module is used to add non-linearity and increase the expressiveness of the transformer model. The two-layer feedforward neural network can learn complex representations of the input tensor, which can be helpful for tasks that require modeling complex relationships between different parts of the input, such as natural language understanding or image processing.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "353cc8cd-ac0e-4295-b060-43bd1d2d08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31655804-c0f6-4d39-b113-3cb0164c8d67",
   "metadata": {},
   "source": [
    "<div>\n",
    "<h3><code>class Block(nn.Module):</code></h3>\n",
    "<p>The Block module is a building block of the transformer model that consists of a multi-head self-attention layer, a feedforward neural network layer, and two layer normalization layers. It takes an input tensor x of shape (B, T, C), where B is the batch size, T is the sequence length, and C is the hidden size of the input tensor.\n",
    "\n",
    "In the __init__ method, the Block module initializes a MultiHeadAttention layer and a FeedFoward layer with the specified embedding dimension n_embd and number of attention heads n_head. It also initializes two LayerNorm layers, which are used to normalize the output of the self-attention and feedforward layers, respectively.\n",
    "\n",
    "In the forward method, the Block module applies the self-attention layer to the input tensor, adds the output of the self-attention layer to the original input tensor using residual connections, normalizes the resulting tensor using layer normalization, applies the feedforward layer to the normalized tensor, adds the output of the feedforward layer to the previous tensor using residual connections, normalizes the resulting tensor using layer normalization, and returns the final tensor.\n",
    "\n",
    "Overall, the Block module is a key building block of the transformer model that allows the model to process sequences of variable length and capture complex relationships between different parts of the input. By stacking multiple Block modules together, the transformer model can learn complex representations of the input sequence that can be used for a wide range of natural language processing tasks, such as machine translation, text classification, and text generation.</p>\n",
    "<h4><code>Parameters:</code></h4>\n",
    "<ul>\n",
    "<li><code>n_embd (int)</code>: The embedding dimension of the input tensor.</li>\n",
    "<li><code>n_head (int)</code>: The number of attention heads to use in the multi-head self-attention layer.</li>\n",
    "</ul>\n",
    "<h4><code>Attributes:</code></h4>\n",
    "<ul>\n",
    "<li><code>sa (MultiHeadAttention)</code>: Multi-head self-attention layer that takes the input tensor as input and returns the output tensor after applying the self-attention mechanism.</li>\n",
    "<li><code>ffwd (FeedFoward)</code>: Feedforward neural network layer that takes the input tensor as input and returns the output tensor after applying a two-layer feedforward neural network.</li>\n",
    "<li><code>ln1 (nn.LayerNorm)</code>: First layer normalization layer that takes the output tensor of the self-attention layer as input and returns the normalized output tensor.</li>\n",
    "<li><code>ln2 (nn.LayerNorm)</code>: Second layer normalization layer that takes the output tensor of the feedforward neural network layer as input and returns the normalized output tensor.</li>\n",
    "</ul>\n",
    "<h4><code>Inputs:</code></h4>\n",
    "<ul>\n",
    "<li><code>x (torch.Tensor)</code>: Input tensor of shape `(B, T, C)`, where `B` is the batch size, `T` is the sequence length, and `C` is the hidden size of the input tensor.</li>\n",
    "</ul>\n",
    "<h4><code>Outputs:</code></h4>\n",
    "<ul>\n",
    "<li><code>x (torch.Tensor)</code>: Output tensor of the same shape as the input tensor.</li>\n",
    "</ul>\n",
    "<h4><code>Methods:</code></h4>\n",
    "<ul>\n",
    "<li><code>forward(x: torch.Tensor) -&gt; torch.Tensor:</code> Computes the output tensor by passing the input tensor through the multi-head self-attention layer, adding the resulting tensor to the input tensor using residual connections, normalizing the resulting tensor using layer normalization, passing the normalized tensor through the feedforward neural network layer, adding the resulting tensor to the previous tensor using residual connections, normalizing the resulting tensor using layer normalization, and returning the final tensor.</li>\n",
    "</ul>\n",
    "<p>Overall, the `Block` module is a key building block of the transformer model that allows the model to process sequences of variable length and capture complex relationships between different parts of the input. By stacking multiple `Block` modules together, the transformer model can learn complex representations of the input sequence that can be used for a wide range of natural language processing tasks, such as machine translation, text classification, and text generation.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d5f8140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc69cfeb-1ef8-4fd9-8eef-1495b7063704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.291345 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c8b18",
   "metadata": {},
   "source": [
    "<h3> 6. Train Bigram Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc04f05d-98c0-48ac-8aea-281b590c6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(filename, model, max_iters, eval_interval, plateau, patience, learning_rate):\n",
    "    # Check if the trained model already exists\n",
    "    if os.path.exists(filename):\n",
    "        use_existing_model = input('Trained model already exists. Do you want to use it? (y/n) ')\n",
    "        if use_existing_model.lower() == 'y':\n",
    "            # Load the existing trained model\n",
    "            model.load_state_dict(torch.load(filename))\n",
    "            return model\n",
    "    \n",
    "    # Train the model\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create a list to store the losses\n",
    "    loss_history = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    num_epochs_no_improvement = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_datetime = datetime.datetime.now(pytz.timezone('US/Pacific'))\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start_time\n",
    "            current_datetime = datetime.datetime.now(pytz.timezone('US/Pacific'))\n",
    "            time_str = current_datetime.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n",
    "\n",
    "            # Store the loss every eval_interval iterations\n",
    "            loss_history.append({'train': losses['train'], 'val': losses['val'], 'step': iter,\n",
    "                                 'timestamp': current_datetime, 'elapsed_time': elapsed_time})\n",
    "\n",
    "            # Check for plateau\n",
    "            if best_val_loss - losses['val'] < plateau:\n",
    "                num_epochs_no_improvement += 1\n",
    "                if num_epochs_no_improvement >= patience:\n",
    "                    print(f\"Validation loss plateaued for {patience} epochs, stopping early...\")\n",
    "                    break\n",
    "            else:\n",
    "                best_val_loss = losses['val']\n",
    "                num_epochs_no_improvement = 0\n",
    "\n",
    "            if iter > 2 * eval_interval:\n",
    "                time_elapsed = current_datetime - start_datetime\n",
    "                time_per_iter = time_elapsed / iter\n",
    "                est_remaining_iters = max_iters - iter\n",
    "                est_remaining_time = est_remaining_iters * time_per_iter\n",
    "                est_completion_time = (current_datetime + est_remaining_time).strftime('%Y-%m-%d %H:%M:%S %Z%z')\n",
    "                print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, timestamp {time_str}, elapsed time {elapsed_time:.4f}s, est. completion {est_completion_time}\")\n",
    "            else:\n",
    "                print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, timestamp {time_str}, elapsed time {elapsed_time:.4f}s\")\n",
    "\n",
    "        if iter == max_iters-1:\n",
    "            print(f\"Training completed: elapsed time {elapsed_time:.4f}s\")\n",
    "\n",
    "    # Plot the loss history\n",
    "    train_losses = [l['train'] for l in loss_history]\n",
    "    val_losses = [l['val'] for l in loss_history]\n",
    "    steps = [l['step'] for l in loss_history]\n",
    "    plt.plot(steps, train_losses, label='Train Loss')\n",
    "    plt.plot(steps, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Validation Loss over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c6c9253",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#filename='shakespeare.pt'\u001b[39;00m\n\u001b[0;32m      2\u001b[0m filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshakespeare2.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplateau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m trained_model\n",
      "Cell \u001b[1;32mIn[19], line 41\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(filename, model, max_iters, eval_interval, plateau, patience, learning_rate)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 41\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     current_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     43\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m current_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m      8\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m----> 9\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     11\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[17], line 38\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,vocab_size)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#filename='shakespeare.pt'\n",
    "filename='shakespeare2.pt'\n",
    "trained_model = train_model(filename, model, max_iters, eval_interval, plateau, patience, learning_rate)\n",
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b4cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_tokens = trained_model.generate(context, max_new_tokens=1000)[0].tolist()\n",
    "generated_text = gpt2enc.decode(generated_tokens)\n",
    "print(generated_text)\n",
    "\n",
    "# Output the generated text to a file\n",
    "with open('generated_text.txt', 'w') as f:\n",
    "    f.write(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83289d-1aed-4c35-9259-267e9f18f5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
